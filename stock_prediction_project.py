# -*- coding: utf-8 -*-
"""Stock Prediction Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18K2VlfA5bh_FfeyzT29AYrLFUW74iCmR

# Overview

If I could predict the stock market with even as much as 30% error, I'd be rich enough to inherit the entire nation of Wakanda, vibranium mountain and all.

There's a reason why an entire district of New York City is dedicated to the art of taming the stock market. Whoever cracks the code even slightly has the potential to get absurd levels of returns, and with those returns, lots of bills.

I'll admit the title of this article is a bit misleading. There is no current method to even remotely accurately predict the stock market, but that's kind of the point.

## Talk about market efficiency and the paradox of passive investors and active investors

## Table of Contents


*   Predicting the market with Stocker
*   Predicting the market with linear regression
*   Predicting the market with neural networks
*   Conclusion -> it's safer to invest in ETFs

# The Stocker Module
"""

!git clone https://github.com/WillKoehrsen/Data-Analysis.git

import os

!ls

os.chdir('Data-Analysis')

!ls

os.chdir('stocker')

!ls

!pip install quandl
!pip install pytrends

import stocker

from stocker import Stocker

goog = Stocker('GOOGL')

goog.plot_stock()

"""If you pay attention, you'll notice that the dates for the Stocker object are not up-to-date. It stops at 2018--3-27. Taking close look at the actual module code, we'll see that the data is taken from Quandl's WIKI exchange. Perhaps the data is not kept up to date?

We can use Stocker to conduct technical stock analysis, but for now we will focus on being mediums. Stocker uses a package created by Facebook called prophet which is good for additive modeling.
"""

model, model_data = goog.create_prophet_model(days=90)
# Let's make some predictions

"""## Evaluation of Predictions

Now we will test the stocker predictions. We need to create a test set and a training set. We'll have our training set to be 2014-2016, and our test set to be 2017. Let's see how accurate this model is.
"""

goog.evaluate_prediction()

"""This is absolutely horrible!

We'll try again, and this time we'll adjust some hyperparameters.

## Adjusting the Changepoint Priors
"""

# changepoint priors is the list of changepoints to evaluate

goog.changepoint_prior_analysis(changepoint_priors=[0.001, 0.05, 0.1, 0.2])

goog.changepoint_prior_validation(start_date='2016-01-04', end_date='2017-01-03', changepoint_priors=[0.001, 0.05, 0.1, 0.2])

"""## Evaluating Refined Model"""

goog.evaluate_prediction()

"""## Testing Our Luck in the Stock Market

Let's see how well our forecasts would play out in the real stock market.
"""

goog.evaluate_prediction(nshares=1000)

"""This shows that it's better to simply invest for the long term.

## Preparing Data for More Machine Learning
"""

# Getting the dataframe of the data

goog_data = goog.make_df('2004-08-19', '2018-03-27')
#2004-08-19 00:00:00 to 2018-03-27 00:00:00.

goog_data.head(50)

goog_data = goog_data[['Date', 'Open', 'High', 'Low', 'Close', 'Adj. Close', 'Volume']]

goog_data.head(50)

"""# Moving Average"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
# %matplotlib inline

import matplotlib.style
import matplotlib as mpl
mpl.style.use('ggplot')

from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 20, 10

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

# Creating copy of goog_data dataframe for moving averages

df = goog_data

df.head()

df['Date'] = pd.to_datetime(df.Date, format='%Y-%m-%d')
df.index = df['Date']

df.head(50)

plt.figure(figsize=(16,8))
plt.plot(df['Date'], df['Adj. Close'], label='Close Price history')

# Creating dataframe with date and the target variable

data = df.sort_index(ascending=True, axis=0)
new_data = pd.DataFrame(index=range(0, len(df)), columns=['Date', 'Adj. Close'])

for i in range(0, len(data)):
  new_data['Date'][i] = data['Date'][i]
  new_data['Adj. Close'][i] = data['Adj. Close'][i]

# Train-test split

train = new_data[:2600]
test = new_data[2600:]

new_data.shape, train.shape, test.shape

num = test.shape[0]

train['Date'].min(), train['Date'].max(), test['Date'].min(), test['Date'].max()

# Making predictions

preds = []
for i in range(0, num):
  a = train['Adj. Close'][len(train)-924+i:].sum() + sum(preds)
  b = a/num
  preds.append(b)

len(preds)

# Measure accuracy with rmse (Root Mean Squared Error)

rms=np.sqrt(np.mean(np.power((np.array(test['Adj. Close'])-preds),2)))

print(rms)

test['Predictions'] = 0
test['Predictions'] = preds
plt.plot(train['Adj. Close'])
plt.plot(test[['Adj. Close', 'Predictions']])

"""# Simple Linear Regression"""

lr_data = goog_data

lr_data.head(50)

# We'll create a separate dataset so that new features don't mess up the original data.


lr_data['Date'] = pd.to_datetime(lr_data.Date, format='%Y-%m-%d')
lr_data.index = lr_data['Date']

lr_data = lr_data.sort_index(ascending=True, axis=0)

new_data = pd.DataFrame(index=range(0, len(lr_data)), columns=['Date', 'Adj. Close'])
for i in range(0,len(data)):
    new_data['Date'][i] = lr_data['Date'][i]
    new_data['Adj. Close'][i] = lr_data['Adj. Close'][i]

new_data.head(50)

!pip install fastai==0.7.0

from fastai.structured import add_datepart

add_datepart(new_data, 'Date')
new_data.drop('Elapsed', axis=1, inplace=True)

new_data.head(50)

# Train-test split

train = new_data[:2600]
test = new_data[2600:]

x_train = train.drop('Adj. Close', axis=1)
y_train = train['Adj. Close']
x_test = test.drop('Adj. Close', axis=1)
y_test = test['Adj. Close']

x_train

# Implementing linear regression
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train, y_train)

# Predictions 
preds = model.predict(x_test)
rms = np.sqrt(np.mean(np.power((np.array(y_test)-np.array(preds)),2)))

print(rms)

# Plot

test['Predictions'] = 0
test['Predictions'] = preds


plt.plot(train['Adj. Close'])
plt.plot(test[['Adj. Close', 'Predictions']])

"""# k-Nearest Neighbours"""

from sklearn import neighbors
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

# scaling the data

x_train_scaled = scaler.fit_transform(x_train)
x_train = pd.DataFrame(x_train_scaled)
x_test_scaled = scaler.fit_transform(x_test)
x_test = pd.DataFrame(x_test_scaled)

# using gridsearch to find the best value of k

params = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}
knn = neighbors.KNeighborsRegressor()
model = GridSearchCV(knn, params, cv=5)

# fitting the model and predicting
model.fit(x_train, y_train)
preds = model.predict(x_test)

# Results

rms = np.sqrt(np.mean(np.power((np.array(y_test)-np.array(preds)),2)))

print(rms)

test['Predictions'] = 0
test['Predictions'] = new_preds


plt.plot(train['Adj. Close'])
plt.plot(test[['Adj. Close', 'Predictions']])

"""# Multilayer Perceptron"""

import tensorflow as tf
from tensorflow.keras import layers

model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Dense(100, activation=tf.nn.relu))

model.add(tf.keras.layers.Dense(100, activation=tf.nn.relu))

model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))

model.compile(optimizer='adam', loss='mean_squared_error')

X_train = np.array(x_train)
Y_train = np.array(y_train)

X_train

X_train.shape

model.fit(X_train, Y_train, epochs=500)

preds = model.predict(x_test)

# Results

rms = np.sqrt(np.mean(np.power((np.array(y_test)-np.array(preds)),2)))

print(rms)

test['Predictions'] = 0
test['Predictions'] = preds


plt.plot(train['Adj. Close'])
plt.plot(test[['Adj. Close', 'Predictions']])